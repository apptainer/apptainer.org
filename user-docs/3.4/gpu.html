

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>GPU Support (NVIDIA CUDA &amp; AMD ROCm) &mdash; Singularity container 3.5 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/ga.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing" href="contributing.html" />
    <link rel="prev" title="Singularity and MPI applications" href="mpi.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Singularity container
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                3.5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to Singularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security in Singularity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_a_container.html">Build a container</a></li>
<li class="toctree-l1"><a class="reference internal" href="definition_files.html">The Definition File</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_env.html">Build Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity_and_docker.html">Singularity and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="fakeroot.html">Fakeroot feature</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="signNverify.html">Sign and Verify</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_commands.html">Key management commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="encryption.html">Encrypted Containers</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="endpoint.html">Remote Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_library.html">Cloud Library</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bind_paths_and_mounts.html">Bind Paths and Mounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="persistent_overlays.html">Persistent Overlays</a></li>
<li class="toctree-l1"><a class="reference internal" href="running_services.html">Running Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_and_metadata.html">Environment and Metadata</a></li>
<li class="toctree-l1"><a class="reference internal" href="oci_runtime.html">OCI Runtime Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="security_options.html">Security Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="networking.html">Network Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="cgroups.html">Cgroups Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Singularity and MPI applications</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPU Support</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#nvidia-gpus-cuda">NVIDIA GPUs &amp; CUDA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#requirements">Requirements</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#library-search-options">Library Search Options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#example-tensorflow-gpu">Example - tensorflow-gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-gpus">Multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cuda-error-unknown-when-everything-seems-to-be-correctly-configured">CUDA_ERROR_UNKNOWN when everything seems to be correctly configured</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#amd-gpus-rocm">AMD GPUs &amp; ROCm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-tensorflow-rocm">Example - tensorflow-rocm</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#opencl-applications">OpenCL Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-blender-opencl">Example - Blender OpenCL</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Reference</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Singularity container</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>GPU Support (NVIDIA CUDA &amp; AMD ROCm)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/sylabs/singularity-userdocs/blob/master/gpu.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gpu-support-nvidia-cuda-amd-rocm">
<span id="gpu"></span><h1>GPU Support (NVIDIA CUDA &amp; AMD ROCm)<a class="headerlink" href="#gpu-support-nvidia-cuda-amd-rocm" title="Permalink to this headline">¶</a></h1>
<p>Singularity natively supports running application containers that use NVIDIA’s
CUDA GPU compute framework, or AMD’s ROCm solution. This allows easy access to
users of GPU-enabled machine learning frameworks such as tensorflow, regardless
of the host operating system. As long as the host has a driver and library
installation for CUDA/ROCm then it’s possible to e.g. run tensorflow in an
up-to-date Ubuntu 18.04 container, from an older RHEL 6 host.</p>
<p>Applications that support OpenCL for compute acceleration can also be used
easily, with an additional bind option.</p>
<div class="section" id="nvidia-gpus-cuda">
<h2>NVIDIA GPUs &amp; CUDA<a class="headerlink" href="#nvidia-gpus-cuda" title="Permalink to this headline">¶</a></h2>
<p>Commands that <code class="docutils literal notranslate"><span class="pre">run</span></code>, or otherwise execute containers (<code class="docutils literal notranslate"><span class="pre">shell</span></code>, <code class="docutils literal notranslate"><span class="pre">exec</span></code>) can
take an <code class="docutils literal notranslate"><span class="pre">--nv</span></code> option, which will setup the container’s environment to use an
NVIDIA GPU and the basic CUDA libraries to run a CUDA enabled application.
The <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag will:</p>
<ul class="simple">
<li><p>Ensure that the <code class="docutils literal notranslate"><span class="pre">/dev/nvidiaX</span></code> device entries are available inside the
container, so that the GPU cards in the host are accessible.</p></li>
<li><p>Locate and bind the basic CUDA libraries from the host into the container, so
that they are available to the container, and match the kernel GPU driver on
the host.</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> inside the container so that the bound-in version
of the CUDA libraries are used by applications run inside the container.</p></li>
</ul>
<div class="section" id="requirements">
<h3>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h3>
<p>To use the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag to run a CUDA application inside a container you must
ensure that:</p>
<ul class="simple">
<li><p>The host has a working installation of the NVIDIA GPU driver, and a matching
version of the basic NVIDIA/CUDA libraries. The host <em>does not</em> need to have an X
server running, unless you want to run graphical apps from the container.</p></li>
<li><p>Either a working installation of the <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> tool is available
on the <code class="docutils literal notranslate"><span class="pre">PATH</span></code> when you run <code class="docutils literal notranslate"><span class="pre">singularity</span></code>, or the NVIDIA libraries are in
the system’s library search path.</p></li>
<li><p>The application inside your container was compiled for a CUDA version, and
device capability level, that is supported by the host card and driver.</p></li>
</ul>
<p>These requirements are usually satisified by installing the NVIDIA drivers and
CUDA packages directly from the NVIDIA website. Linux distributions may provide
NVIDIA drivers and CUDA libraries, but they are often outdated which can lead to
problems running applications compiled for the latest versions of CUDA.</p>
<div class="section" id="library-search-options">
<h4>Library Search Options<a class="headerlink" href="#library-search-options" title="Permalink to this headline">¶</a></h4>
<p>Singularity will find the NVIDIA/CUDA libraries on your host either using the
<code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> tool, or, if it is not available, a list of libraries
in the configuration file <code class="docutils literal notranslate"><span class="pre">etc/singularity/nvbliblist</span></code>.</p>
<p>If possible we recommend installing the <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> tool from the
<a class="reference external" href="https://nvidia.github.io/libnvidia-container/">NVIDIA libnvidia-container website</a></p>
<p>The fall-back <code class="docutils literal notranslate"><span class="pre">etc/singularity/nvbliblist</span></code> library list is correct at time of
release for CUDA 10.1. However, if future CUDA versions split or add library files
you may need to edit it. The <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> tool will be updated by
NVIDIA to always return the appropriate list of libraries.</p>
</div>
</div>
<div class="section" id="example-tensorflow-gpu">
<h3>Example - tensorflow-gpu<a class="headerlink" href="#example-tensorflow-gpu" title="Permalink to this headline">¶</a></h3>
<p>Tensorflow is commonly used for machine learning projects but can be diffficult
to install on older systems, and is updated frequently. Running tensorflow from
a container removes installation problems and makes trying out new versions easy.</p>
<p>The official tensorflow repository on Docker Hub contains NVIDA GPU supporting
containers, that will use CUDA for processing. You can view the available
versions on the <a class="reference external" href="https://hub.docker.com/r/tensorflow/tensorflow/tags">tags page on Docker Hub</a></p>
<p>The container is large, so it’s best to build or pull the docker image to a SIF
before you start working with it:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity pull docker://tensorflow/tensorflow:latest-gpu
...
INFO:    Creating SIF file...
INFO:    Build complete: tensorflow_latest-gpu.sif
</pre></div>
</div>
<p>Then run the container with GPU support:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity run --nv tensorflow_latest-gpu.sif

________                               _______________
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

Singularity&gt;
</pre></div>
</div>
<p>You can verify the GPU is available within the container by using the
tensorflow <code class="docutils literal notranslate"><span class="pre">list_local_devices()</span></code> function:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Singularity&gt; python
Python 2.7.15+ (default, Jul  9 2019, 16:51:35)
[GCC 7.4.0] on linux2
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from tensorflow.python.client import device_lib
&gt;&gt;&gt; print(device_lib.list_local_devices())
2019-11-14 15:32:09.743600: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-14 15:32:09.784482: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3292620000 Hz
2019-11-14 15:32:09.787911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565246634360 executing computations on platform Host. Devices:
2019-11-14 15:32:09.787939: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-14 15:32:09.798428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-14 15:32:09.842683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-14 15:32:09.843252: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5652469263d0 executing computations on platform CUDA. Devices:
2019-11-14 15:32:09.843265: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GT 730, Compute Capability 3.5
2019-11-14 15:32:09.843380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-14 15:32:09.843984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015
...
</pre></div>
</div>
</div>
<div class="section" id="multiple-gpus">
<h3>Multiple GPUs<a class="headerlink" href="#multiple-gpus" title="Permalink to this headline">¶</a></h3>
<p>By default, Singularity makes all host devices available in the container. When
the <code class="docutils literal notranslate"><span class="pre">--contain</span></code> option is used a minimal <code class="docutils literal notranslate"><span class="pre">/dev</span></code> tree is created in the
container, but the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> option will ensure that all nvidia devices on the
host are present in the container.</p>
<p>This behaviour is different to <code class="docutils literal notranslate"><span class="pre">nvidia-docker</span></code> where an <code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code>
environment variable is used to control whether some or all host GPUs are visible
inside a container. The <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code> explicitly binds the devices
into the container dependent on the value of <code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code>.</p>
<p>To control which GPUs are used in a Singularity container that is run with
<code class="docutils literal notranslate"><span class="pre">--nv</span></code> you can set <code class="docutils literal notranslate"><span class="pre">SINGULARITYENV_CUDA_VISIBLE_DEVICES</span></code> before running the
container, or <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> inside the container.  This variable will
limit the GPU devices that CUDA programs see.</p>
<p>E.g. to run the tensorflow container, but using only the first GPU in the host,
we could do:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ SINGULARITYENV_CUDA_VISIBLE_DEVICES=0 singularity run --nv tensorflow_latest-gpu.sif

# or

$ export SINGULARITYENV_CUDA_VISIBLE_DEVICES=0
$ singularity run tensorflow_latest-gpu.sif
</pre></div>
</div>
</div>
<div class="section" id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h3>
<p>If the host installation of the NVIDIA / CUDA driver and libraries is working
and up-to-date there are rarely issues running CUDA programs inside of
Singularity containers. The most common issue seen is:</p>
<div class="section" id="cuda-error-unknown-when-everything-seems-to-be-correctly-configured">
<h4>CUDA_ERROR_UNKNOWN when everything seems to be correctly configured<a class="headerlink" href="#cuda-error-unknown-when-everything-seems-to-be-correctly-configured" title="Permalink to this headline">¶</a></h4>
<p>CUDA depends on multiple kernel modules being loaded. Not all of the modules are
loaded at system startup. Some portions of the NVIDA driver stack are initialized
when first needed. This is done using a setuid root binary, so initializing can
be triggered by any user on the host. In Singularity containers, privilege
escalation is blocked, so the setuid root binary cannot initialize the driver
stack fully.</p>
<p>If you experience <code class="docutils literal notranslate"><span class="pre">CUDA_ERROR_UNKNOWN</span></code> in a container, initialize the driver
stack on the host first, by running a CUDA program there or
<code class="docutils literal notranslate"><span class="pre">modprobe</span> <span class="pre">nvidia_uvm</span></code> as root, and using <code class="docutils literal notranslate"><span class="pre">nvidia-persistenced</span></code> to avoid
driver unload.</p>
</div>
</div>
</div>
<div class="section" id="amd-gpus-rocm">
<h2>AMD GPUs &amp; ROCm<a class="headerlink" href="#amd-gpus-rocm" title="Permalink to this headline">¶</a></h2>
<p>Singularity 3.5 adds a <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> flag to support GPU compute with the ROCm
framework using AMD Radeon GPU cards.</p>
<p>Commands that <code class="docutils literal notranslate"><span class="pre">run</span></code>, or otherwise execute containers (<code class="docutils literal notranslate"><span class="pre">shell</span></code>, <code class="docutils literal notranslate"><span class="pre">exec</span></code>) can
take an <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> option, which will setup the container’s environment to use a
Radeon GPU and the basic ROCm libraries to run a ROCm enabled application.
The <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> flag will:</p>
<ul class="simple">
<li><p>Ensure that the <code class="docutils literal notranslate"><span class="pre">/dev/dri/</span></code> device entries are available inside the
container, so that the GPU cards in the host are accessible.</p></li>
<li><p>Locate and bind the basic ROCm libraries from the host into the container, so
that they are available to the container, and match the kernel GPU driver on
the host.</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> inside the container so that the bound-in version
of the ROCm libraries are used by application run inside the container.</p></li>
</ul>
<div class="section" id="id1">
<h3>Requirements<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>To use the <code class="docutils literal notranslate"><span class="pre">--rcom</span></code> flag to run a CUDA application inside a container you must
ensure that:</p>
<ul class="simple">
<li><p>The host has a working installation of the <code class="docutils literal notranslate"><span class="pre">amdgpu</span></code> driver, and a compatible
version of the basic ROCm libraries. The host <em>does not</em> need to have an X
server running, unless you want to run graphical apps from the container.</p></li>
<li><p>The ROCm libraries are in the system’s library search path.</p></li>
<li><p>The application inside your container was compiled for a ROCm version that is
compatible with the ROCm version on your host.</p></li>
</ul>
<p>These requirements can be satisfied by following the requirements on the
<a class="reference external" href="https://rocm.github.io/ROCmInstall.html">ROCm web site</a></p>
<p>At time of release, Singularity was tested successfully on Debian 10 with ROCm
2.8/2.9 and the upstream kernel driver, and Ubuntu 18.04 with ROCm 2.9 and the
DKMS driver.</p>
</div>
<div class="section" id="example-tensorflow-rocm">
<h3>Example - tensorflow-rocm<a class="headerlink" href="#example-tensorflow-rocm" title="Permalink to this headline">¶</a></h3>
<p>Tensorflow is commonly used for machine learning projects, but can be difficult
to install on older systems, and is updated frequently. Running tensorflow from
a container removes installation problems and makes trying out new versions easy.</p>
<p>The rocm tensorflow repository on Docker Hub contains Radeon GPU supporting
containers, that will use ROCm for processing. You can view the available
versions on the <a class="reference external" href="https://hub.docker.com/r/rocm/tensorflow/tags">tags page on Docker Hub</a></p>
<p>The container is large, so it’s best to build or pull the docker image to a SIF
before you start working with it:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity pull docker://rocm/tensorflow:latest
...
INFO:    Creating SIF file...
INFO:    Build complete: tensorflow_latest.sif
</pre></div>
</div>
<p>Then run the container with GPU support:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity run --rocm tensorflow_latest.sif
</pre></div>
</div>
<p>You can verify the GPU is available within the container by using the
tensorflow <code class="docutils literal notranslate"><span class="pre">list_local_devices()</span></code> function:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Singularity&gt; ipython
Python 3.5.2 (default, Jul 10 2019, 11:58:48)
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 7.8.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.
&gt;&gt;&gt; from tensorflow.python.client import device_lib
...
&gt;&gt;&gt; print(device_lib.list_local_devices())
...
2019-11-14 16:33:42.750509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1651] Found device 0 with properties:
name: Lexa PRO [Radeon RX 550/550X]
AMDGPU ISA: gfx803
memoryClockRate (GHz) 1.183
pciBusID 0000:09:00.0
...
</pre></div>
</div>
</div>
</div>
<div class="section" id="opencl-applications">
<h2>OpenCL Applications<a class="headerlink" href="#opencl-applications" title="Permalink to this headline">¶</a></h2>
<p>Both the <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> and <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flags will bind the vendor OpenCL implementation
libraries into a container that is being run. However, these libraries will not
be used by OpenCL applications unless a vendor icd file is available under
<code class="docutils literal notranslate"><span class="pre">/etc/OpenCL/vendors</span></code> that directs OpenCL to use the vendor library.</p>
<p>The simplest way to use OpenCL in a container is to <code class="docutils literal notranslate"><span class="pre">--bind</span> <span class="pre">/etc/OpenCL</span></code> so
that the icd files from the host (which match the bound-in libraries) are
present in the container.</p>
<div class="section" id="example-blender-opencl">
<h3>Example - Blender OpenCL<a class="headerlink" href="#example-blender-opencl" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://github.com/sylabs/examples">Sylabs examples repository</a> contains
an example container definition for the 3D modelling application ‘Blender’.</p>
<p>The latest versions of Blender supports OpenCL rendering. You can run Blender
as a graphical application that will make use of a local Radeon GPU for OpenCL
compute using the container that has been published to the Sylabs library:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ singularity exec --rocm --bind /etc/OpenCL library://sylabs/examples/blender blender
</pre></div>
</div>
<p>Note the <em>exec</em> used as the <em>runscript</em> for this container is setup for batch
rendering (which can also use OpenCL).</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="contributing.html" class="btn btn-neutral float-right" title="Contributing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mpi.html" class="btn btn-neutral float-left" title="Singularity and MPI applications" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017-2019, Sylabs Inc.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>