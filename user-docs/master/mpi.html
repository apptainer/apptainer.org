<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Singularity and MPI applications &mdash; Singularity User Guide 3.8 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/ga.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GPU Support (NVIDIA CUDA &amp; AMD ROCm)" href="gpu.html" />
    <link rel="prev" title="Limiting container resources with cgroups" href="cgroups.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Singularity User Guide
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                3.8
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to Singularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security in Singularity</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_a_container.html">Build a container</a></li>
<li class="toctree-l1"><a class="reference internal" href="definition_files.html">The Definition File</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_env.html">Build Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity_and_docker.html">Singularity and Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="fakeroot.html">Fakeroot feature</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="signNverify.html">Sign and Verify</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_commands.html">Key management commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="encryption.html">Encrypted Containers</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="endpoint.html">Remote Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_library.html">Cloud Library</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bind_paths_and_mounts.html">Bind Paths and Mounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="persistent_overlays.html">Persistent Overlays</a></li>
<li class="toctree-l1"><a class="reference internal" href="running_services.html">Running Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_and_metadata.html">Environment and Metadata</a></li>
<li class="toctree-l1"><a class="reference internal" href="oci_runtime.html">OCI Runtime Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="security_options.html">Security Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="networking.html">Network Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="cgroups.html">Cgroups Support</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Singularity and MPI applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hybrid-model">Hybrid model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#test-application">Test Application</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mpich-hybrid-container">MPICH Hybrid Container</a></li>
<li class="toctree-l3"><a class="reference internal" href="#open-mpi-hybrid-container">Open MPI Hybrid Container</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-an-mpi-application">Running an MPI Application</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#bind-model">Bind model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bind-mode-definition-file">Bind Mode Definition File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Running an MPI Application</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#batch-scheduler-slurm">Batch Scheduler / Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alternative-launchers">Alternative Launchers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interconnects-networking">Interconnects / Networking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting-tips">Troubleshooting Tips</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU Support</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Singularity User Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Singularity and MPI applications</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/hpcng/singularity-userdocs/blob/master/mpi.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="singularity-and-mpi-applications">
<span id="mpi"></span><h1>Singularity and MPI applications<a class="headerlink" href="#singularity-and-mpi-applications" title="Permalink to this headline"></a></h1>
<p id="sec-mpi">The <a class="reference external" href="https://mpi-forum.org">Message Passing Interface (MPI)</a>
is a standard extensively used by HPC applications to implement various communication
across compute nodes of a single system or across compute platforms.
There are two main open-source implementations of MPI at the
moment - <a class="reference external" href="https://www.open-mpi.org/">OpenMPI</a> and <a class="reference external" href="https://www.mpich.org/">MPICH</a>,
both of which are supported by Singularity. The goal of this page is to
demonstrate the development and running of MPI programs using Singularity containers.</p>
<p>There are several ways of carrying this out, the most popular way of
executing MPI applications installed in a Singularity container is to rely on the
MPI implementation available on the host. This is called the <em>Host MPI</em> or
the <em>Hybrid</em> model since both the MPI implementations provided by system
administrators (on the host) and in the containers will be used.</p>
<p>Another approach is to only use the MPI implementation available on the host and
not include any MPI in the container. This is called the <em>Bind</em> model since it
requires to bind/mount the MPI version available on the host into the container.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <em>bind</em> model requires users to be able to mount user-specified
files from the host into the container. This ability is sometimes
disabled by system administrators for operational reasons. If this
is the case on your system please follow the <em>hybrid</em> approach.</p>
</div>
<section id="hybrid-model">
<h2>Hybrid model<a class="headerlink" href="#hybrid-model" title="Permalink to this headline"></a></h2>
<p>The basic idea behind the <em>Hybrid Approach</em> is when you execute a Singularity
container with MPI code, you will call <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> or a similar launcher on the
<code class="docutils literal notranslate"><span class="pre">singularity</span></code> command itself. The MPI process outside of the container will
then work in tandem with MPI inside the container and the containerized MPI code
to instantiate the job.</p>
<p>The Open MPI/Singularity workflow in detail:</p>
<ol class="arabic simple">
<li><p>The MPI launcher (e.g., <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>, <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>) is called by the resource manager or the user directly from a shell.</p></li>
<li><p>Open MPI then calls the process management daemon (ORTED).</p></li>
<li><p>The ORTED process launches the Singularity container requested by the launcher command.</p></li>
<li><p>Singularity instantiates the container and namespace environment.</p></li>
<li><p>Singularity then launches the MPI application within the container.</p></li>
<li><p>The MPI application launches and loads the Open MPI libraries.</p></li>
<li><p>The Open MPI libraries connect back to the ORTED process via the Process Management Interface (PMI).</p></li>
</ol>
<p>At this point the processes within the container run as they would normally directly on the host.</p>
<dl class="simple">
<dt>The advantages of this approach are:</dt><dd><ul class="simple">
<li><p>Integration with resource managers such as Slurm.</p></li>
<li><p>Simplicity since similar to natively running MPI applications.</p></li>
</ul>
</dd>
<dt>The drawbacks are:</dt><dd><ul class="simple">
<li><p>The MPI in the container must be compatible with the version of MPI
available on the host.</p></li>
<li><p>The MPI implementation in the container must be carefully
configured for optimal use of the hardware if performance is
critical.</p></li>
</ul>
</dd>
</dl>
<p>Since the MPI implementation in the container must be compliant with
the version available on the host system, a standard approach is to
build your own MPI container, including building the same MPI
framework installed on the host from source.</p>
<section id="test-application">
<h3>Test Application<a class="headerlink" href="#test-application" title="Permalink to this headline"></a></h3>
<p>To illustrate how Singularity can be used to execute MPI applications, we will
assume for a moment that the application is <code class="docutils literal notranslate"><span class="pre">mpitest.c</span></code>, a simple Hello World:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mpi.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdlib.h&gt;</span><span class="cp"></span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">rc</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">myrank</span><span class="p">;</span><span class="w"></span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Init</span><span class="w"> </span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Init() failed&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_FAILURE</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_size</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Comm_size() failed&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">                </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>

<span class="w">        </span><span class="n">rc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MPI_Comm_rank</span><span class="w"> </span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myrank</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rc</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">MPI_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">                </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MPI_Comm_rank() failed&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">                </span><span class="k">goto</span><span class="w"> </span><span class="n">exit_with_error</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>

<span class="w">        </span><span class="n">fprintf</span><span class="w"> </span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Hello, I am rank %d/%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">myrank</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span><span class="w"></span>

<span class="w">        </span><span class="n">MPI_Finalize</span><span class="p">();</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">;</span><span class="w"></span>

<span class="w"> </span><span class="nl">exit_with_error</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="n">MPI_Finalize</span><span class="p">();</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_FAILURE</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MPI is an interface to a library, so it consists of function calls and
libraries that can be used by many programming languages. It comes with
standardized bindings for Fortran and C. However, it can support
applications in many languages like Python, R, etc.</p>
</div>
<p>The next step is to create the definition file used to build the
container, which will depend on the MPI implementation available on
the host.</p>
</section>
<section id="mpich-hybrid-container">
<h3>MPICH Hybrid Container<a class="headerlink" href="#mpich-hybrid-container" title="Permalink to this headline"></a></h3>
<p>If the host MPI is MPICH, a definition file such as the following example can be used:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Bootstrap: docker
From: ubuntu:18.04

%files
    mpitest.c /opt

%environment
    # Point to MPICH binaries, libraries man pages
    export MPICH_DIR=/opt/mpich-3.3.2
    export PATH=&quot;$MPICH_DIR/bin:$PATH&quot;
    export LD_LIBRARY_PATH=&quot;$MPICH_DIR/lib:$LD_LIBRARY_PATH&quot;
    export MANPATH=$MPICH_DIR/share/man:$MANPATH

%post
    echo &quot;Installing required packages...&quot;
    export DEBIAN_FRONTEND=noninteractive
    apt-get update &amp;&amp; apt-get install -y wget git bash gcc gfortran g++ make

    # Information about the version of MPICH to use
    export MPICH_VERSION=3.3.2
    export MPICH_URL=&quot;http://www.mpich.org/static/downloads/$MPICH_VERSION/mpich-$MPICH_VERSION.tar.gz&quot;
    export MPICH_DIR=/opt/mpich

    echo &quot;Installing MPICH...&quot;
    mkdir -p /tmp/mpich
    mkdir -p /opt
    # Download
    cd /tmp/mpich &amp;&amp; wget -O mpich-$MPICH_VERSION.tar.gz $MPICH_URL &amp;&amp; tar xzf mpich-$MPICH_VERSION.tar.gz
    # Compile and install
    cd /tmp/mpich/mpich-$MPICH_VERSION &amp;&amp; ./configure --prefix=$MPICH_DIR &amp;&amp; make install

    # Set env variables so we can compile our application
    export PATH=$MPICH_DIR/bin:$PATH
    export LD_LIBRARY_PATH=$MPICH_DIR/lib:$LD_LIBRARY_PATH

    echo &quot;Compiling the MPI application...&quot;
    cd /opt &amp;&amp; mpicc -o mpitest mpitest.c
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The version of MPICH you install in the container must be
compatible with the version on the host. It should also be
configured to support the same process management mechanism and
version, e.g. PMI2 / PMIx, as used on the host.</p>
<p>There are wide variations in MPI configuration across HPC
systems. Consult your system documentation, or ask your support
staff for details.</p>
</div>
</section>
<section id="open-mpi-hybrid-container">
<h3>Open MPI Hybrid Container<a class="headerlink" href="#open-mpi-hybrid-container" title="Permalink to this headline"></a></h3>
<p>If the host MPI is Open MPI, the definition file looks like:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Bootstrap: docker
From: ubuntu:18.04

%files
    mpitest.c /opt

%environment
    # Point to OMPI binaries, libraries, man pages
    export OMPI_DIR=/opt/ompi
    export PATH=&quot;$OMPI_DIR/bin:$PATH&quot;
    export LD_LIBRARY_PATH=&quot;$OMPI_DIR/lib:$LD_LIBRARY_PATH&quot;
    export MANPATH=&quot;$OMPI_DIR/share/man:$MANPATH&quot;

%post
    echo &quot;Installing required packages...&quot;
    apt-get update &amp;&amp; apt-get install -y wget git bash gcc gfortran g++ make file

    echo &quot;Installing Open MPI&quot;
    export OMPI_DIR=/opt/ompi
    export OMPI_VERSION=4.0.5
    export OMPI_URL=&quot;https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-$OMPI_VERSION.tar.bz2&quot;
    mkdir -p /tmp/ompi
    mkdir -p /opt
    # Download
    cd /tmp/ompi &amp;&amp; wget -O openmpi-$OMPI_VERSION.tar.bz2 $OMPI_URL &amp;&amp; tar -xjf openmpi-$OMPI_VERSION.tar.bz2
    # Compile and install
    cd /tmp/ompi/openmpi-$OMPI_VERSION &amp;&amp; ./configure --prefix=$OMPI_DIR &amp;&amp; make -j8 install

    # Set env variables so we can compile our application
    export PATH=$OMPI_DIR/bin:$PATH
    export LD_LIBRARY_PATH=$OMPI_DIR/lib:$LD_LIBRARY_PATH

    echo &quot;Compiling the MPI application...&quot;
    cd /opt &amp;&amp; mpicc -o mpitest mpitest.c
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The version of Open MPI you install in the container must be
compatible with the version on the host. It should also be
configured to support the same process management mechanism and
version, e.g. PMI2 / PMIx, as used on the host.</p>
<p>There are wide variations in MPI configuration across HPC
systems. Consult your system documentation, or ask your support
staff for details.</p>
</div>
</section>
<section id="running-an-mpi-application">
<h3>Running an MPI Application<a class="headerlink" href="#running-an-mpi-application" title="Permalink to this headline"></a></h3>
<p>The standard way to execute MPI applications with hybrid Singularity containers is to
run the native <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command from the host, which will start Singularity
containers and ultimately MPI ranks within the containers.</p>
<p>Assuming your container with MPI and your application is already built,
the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command to start your application looks like when your container
has been built based on the hybrid model:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpirun -n &lt;NUMBER_OF_RANKS&gt; singularity exec &lt;PATH/TO/MY/IMAGE&gt; &lt;/PATH/TO/BINARY/WITHIN/CONTAINER&gt;
</pre></div>
</div>
<p>Practically, this command will first start a process instantiating <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>
and then Singularity containers on compute nodes. Finally, when the containers
start, the MPI binary is executed:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpirun -n 8 singularity run hybrid-mpich.sif /opt/mpitest
Hello, I am rank 3/8
Hello, I am rank 4/8
Hello, I am rank 6/8
Hello, I am rank 2/8
Hello, I am rank 0/8
Hello, I am rank 5/8
Hello, I am rank 1/8
Hello, I am rank 7/8
</pre></div>
</div>
</section>
</section>
<section id="bind-model">
<h2>Bind model<a class="headerlink" href="#bind-model" title="Permalink to this headline"></a></h2>
<p>Similar to the <em>Hybrid Approach</em>, the basic idea behind the <em>Bind Approach</em> is to start the MPI
application by calling the MPI launcher (e.g., <cite>mpirun</cite>) from the host. The main difference between
the hybrid and bind approach is the fact that with the bind approach, the container usually does
not include any MPI implementation. This means that Singularity needs to mount/bind the MPI
available on the host into the container.</p>
<p>Technically this requires two steps:</p>
<ol class="arabic simple">
<li><p>Know where the MPI implementation on the host is installed.</p></li>
<li><p>Mount/bind it into the container in a location where the system will be able to find libraries and binaries.</p></li>
</ol>
<dl class="simple">
<dt>The advantages of this approach are:</dt><dd><ul class="simple">
<li><p>Integration with resource managers such as Slurm.</p></li>
<li><p>Container images are smaller since there is no need to add an MPI in the containers.</p></li>
</ul>
</dd>
<dt>The drawbacks are:</dt><dd><ul class="simple">
<li><p>The MPI used to compile the application in the container must be compatible with
the version of MPI available on the host.</p></li>
<li><p>The user must know where the host MPI is installed.</p></li>
<li><p>The user must ensure that binding the directory where the host MPI is installed is
possible.</p></li>
<li><p>The user must ensure that the host MPI is compatible with the MPI used to compile
and install the application in the container.</p></li>
</ul>
</dd>
</dl>
<p>The creation of a Singularity container for the bind model is based on the following steps:</p>
<ol class="arabic simple">
<li><p>Compile your application on a system with the target MPI implementation, as you would do
to install your application on any system.</p></li>
<li><p>Create a definition file that includes the copy of the application from the host to the container
image, as well as all required dependencies.</p></li>
<li><p>Generate the container image.</p></li>
</ol>
<p>As already mentioned, the compilation of the application on the host is not different from
the installation of your application on any system. Just make sure that the MPI on the system
where you create your container is compatible with the MPI available on the platform(s) where
you want to run your containers. For example, a container where the application has been compiled
with MPICH will not be able to run on a system where only Open MPI is available, even if you mount
the directory where Open MPI is installed.</p>
<section id="bind-mode-definition-file">
<h3>Bind Mode Definition File<a class="headerlink" href="#bind-mode-definition-file" title="Permalink to this headline"></a></h3>
<p>A definition file for a container in bind mode is fairly straight
forward. The following example shows the definition file for the test
program, which in this case has been compiled on the host to
<code class="docutils literal notranslate"><span class="pre">/tmp/mpitest</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Bootstrap: docker
From: ubuntu:18.04

%files
      /tmp/mpitest /opt/mpitest

%environment
      export PATH=&quot;$MPI_DIR/bin:$PATH&quot;
      export LD_LIBRARY_PATH=&quot;$MPI_DIR/lib:$LD_LIBRARY_PATH&quot;
</pre></div>
</div>
<p>In this example, the application <code class="docutils literal notranslate"><span class="pre">mpitest</span></code> is copied from the host
into <code class="docutils literal notranslate"><span class="pre">/opt</span></code>, so we will need to run it as <code class="docutils literal notranslate"><span class="pre">/opt/mpitest</span></code> inside
our container.</p>
<p>The environment section adds paths for binaries and libraries under
<code class="docutils literal notranslate"><span class="pre">$MPI_DIR</span></code> - which we will need to set when running the container.</p>
</section>
<section id="id1">
<h3>Running an MPI Application<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>When running our bind mode container we need to <code class="docutils literal notranslate"><span class="pre">--bind</span></code> our host’s
MPI installation into the container. We also need to set the
environment variable <code class="docutils literal notranslate"><span class="pre">$MPI_DIR</span></code> in the container to point to the
location where the MPI installation is bound in.</p>
<p>Setting up the container in this way makes it semi-portable between
systems that have a version-compatible MPI installation, but under
different installation paths. You can also hard code the MPI path in
the definition file if you wish.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ export MPI_DIR=&quot;&lt;PATH/TO/HOST/MPI/DIRECTORY&gt;&quot;
$ mpirun -n &lt;NUMBER_OF_RANKS&gt; singularity exec --bind &quot;$MPI_DIR&quot; &lt;PATH/TO/MY/IMAGE&gt; &lt;/PATH/TO/BINARY/WITHIN/CONTAINER&gt;
</pre></div>
</div>
<p>On an example system we may be using an Open MPI installation at
<code class="docutils literal notranslate"><span class="pre">/cm/shared/apps/openmpi/gcc/64/4.0.5/</span></code>. This means that the
commands to run the container in bind mode are:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ export MPI_DIR=&quot;/cm/shared/apps/openmpi/gcc/64/4.0.5&quot;
$ mpirun -n 8 singularity exec --bind &quot;$MPI_DIR&quot; bind.sif /opt/mpitest
Hello, I am rank 1/8
Hello, I am rank 2/8
Hello, I am rank 0/8
Hello, I am rank 7/8
Hello, I am rank 5/8
Hello, I am rank 3/8
Hello, I am rank 4/8
Hello, I am rank 6/8
</pre></div>
</div>
</section>
</section>
<section id="batch-scheduler-slurm">
<h2>Batch Scheduler / Slurm<a class="headerlink" href="#batch-scheduler-slurm" title="Permalink to this headline"></a></h2>
<p>If your target system is setup with a batch system such as SLURM, a standard
way to execute MPI applications is through a batch script. The following
example illustrates the context of a batch script for Slurm that aims at
starting a Singularity container on each node allocated to the execution of
the job. It can easily be adapted for all major batch systems available.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ cat my_job.sh
#!/bin/bash
#SBATCH --job-name singularity-mpi
#SBATCH -N $NNODES # total number of nodes
#SBATCH --time=00:05:00 # Max execution time

mpirun -n $NP singularity exec /var/nfsshare/gvallee/mpich.sif /opt/mpitest
</pre></div>
</div>
<p>In fact, the example describes a job that requests the number of nodes specified
by the <code class="docutils literal notranslate"><span class="pre">NNODES</span></code> environment variable and a total number of MPI processes specified
by the <code class="docutils literal notranslate"><span class="pre">NP</span></code> environment variable. The example is also assuming that the container
is based on the hybrid model; if it is based on the bind model, please add the
appropriate bind options.</p>
<p>A user can then submit a job by executing the following SLURM command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ sbatch my_job.sh
</pre></div>
</div>
</section>
<section id="alternative-launchers">
<h2>Alternative Launchers<a class="headerlink" href="#alternative-launchers" title="Permalink to this headline"></a></h2>
<p>On many systems it is common to use an alternative launcher to start
MPI applications, e.g. Slurm’s <code class="docutils literal notranslate"><span class="pre">srun</span></code> rather than the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>
provided by the MPI installation. This approach is supported with
Singularity as long as the container MPI version supports the same
process management interface (e.g. PMI2 / PMIx) and version as is used
by the launcher.</p>
<p>In the bind mode the host MPI is used in the container, and should
interact correctly with the same launchers as it does on the host.</p>
</section>
<section id="interconnects-networking">
<h2>Interconnects / Networking<a class="headerlink" href="#interconnects-networking" title="Permalink to this headline"></a></h2>
<p>High performance interconnects such as Infiniband and Omnipath require
that MPI implementations are built to support them. You may need to
install or bind Infiniband/Omnipath libraries into your containers
when using these interconnects.</p>
<p>By default Singularity exposes every device in <code class="docutils literal notranslate"><span class="pre">/dev</span></code> to the
container. If you run a container using the <code class="docutils literal notranslate"><span class="pre">--contain</span></code> or
<code class="docutils literal notranslate"><span class="pre">--containall</span></code> flags a minimal <code class="docutils literal notranslate"><span class="pre">/dev</span></code> is used instead. You may
need to bind in additional <code class="docutils literal notranslate"><span class="pre">/dev/</span></code> entries manually to
support the operation of your interconnect drivers in the container in
this case.</p>
</section>
<section id="troubleshooting-tips">
<h2>Troubleshooting Tips<a class="headerlink" href="#troubleshooting-tips" title="Permalink to this headline"></a></h2>
<p>If your containers run N rank 0 processes, instead of operating
correctly as an MPI application, it is likely that the MPI stack used
to launch the containerized application is not compatible with, or
cannot communicate with, the MPI stack in the container.</p>
<p>E.g. if we attempt to run the hybrid Open MPI container, but with
<code class="docutils literal notranslate"><span class="pre">mpirun</span></code> from MPICH loaded on the host:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ module add mpich
$ mpirun -n 8 singularity run hybrid-openmpi.sif /opt/mpitest
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
Hello, I am rank 0/1
</pre></div>
</div>
<p>If your container starts processes of different ranks, but fails with
communications errors there may also be a version incompatibility, or
interconnect libraries may not be available or configured properly
with the MPI stack in the container.</p>
<p>Please check the following things carefully before asking questions in
the Singularity community:</p>
<blockquote>
<div><ul class="simple">
<li><p>For the hybrid mode, is the MPI version on the host compatible with
the version in the container? Newer MPI versions can generally
tolerate some mismatch in the version number, but it is safest to
use identical versions.</p></li>
<li><p>Is the MPI stack in the container configured to support the process
management method used on the host? E.g. if you are launching tasks
with <code class="docutils literal notranslate"><span class="pre">srun</span></code> configured for PMIx only, then a containerized MPI
supporting PMI2 only will not operate as expected.</p></li>
<li><p>If you are using an interconnect other than standard Ethernet, are
any required libraries for it installed or bound into the
container? Is the MPI stack in the container configured correctly
to use them?</p></li>
</ul>
</div></blockquote>
<p>We recommend using the Singularity Google Group or Slack Channel to
ask for MPI advice from the Singularity community. HPC cluster
configurations vary greatly and most MPI problems are related to MPI /
interconnect configuration, and not caused by issues in Singularity
itself.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cgroups.html" class="btn btn-neutral float-left" title="Limiting container resources with cgroups" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gpu.html" class="btn btn-neutral float-right" title="GPU Support (NVIDIA CUDA &amp; AMD ROCm)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017-2021, HPCng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>