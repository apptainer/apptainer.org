<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPU Support (NVIDIA CUDA &amp; AMD ROCm) &mdash; Apptainer User Guide main documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/js/ga.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing" href="contributing.html" />
    <link rel="prev" title="Apptainer and MPI applications" href="mpi.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Apptainer User Guide
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                main
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to Apptainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security in Apptainer</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="build_a_container.html">Build a container</a></li>
<li class="toctree-l1"><a class="reference internal" href="definition_files.html">The Definition File</a></li>
<li class="toctree-l1"><a class="reference internal" href="build_env.html">Build Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="fakeroot.html">Fakeroot feature</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="signNverify.html">Sign and Verify</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_commands.html">Key management commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="encryption.html">Encrypted Containers</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="endpoint.html">Remote Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="library_api.html">Library API Registries</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bind_paths_and_mounts.html">Bind Paths and Mounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="persistent_overlays.html">Persistent Overlays</a></li>
<li class="toctree-l1"><a class="reference internal" href="running_services.html">Instances - Running Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_and_metadata.html">Environment and Metadata</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="security_options.html">Security Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="networking.html">Network Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="cgroups.html">Limiting Container Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">Application Checkpointing</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="singularity_compatibility.html">Singularity Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="docker_and_oci.html">Support for Docker / OCI Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="oci_runtime.html">OCI Runtime Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Apptainer and MPI applications</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPU Support</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#nvidia-gpus-cuda-standard">NVIDIA GPUs &amp; CUDA (Standard)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#requirements">Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-tensorflow-gpu">Example - tensorflow-gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-gpus">Multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cuda-error-unknown-when-everything-seems-to-be-correctly-configured">CUDA_ERROR_UNKNOWN when everything seems to be correctly configured</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nvidia-gpus-cuda-nvidia-container-cli">NVIDIA GPUs &amp; CUDA (nvidia-container-cli)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#requirements-limitations">Requirements &amp; Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Example - tensorflow-gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-selection">GPU Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-gpu-options">Other GPU Options</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#amd-gpus-rocm">AMD GPUs &amp; ROCm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-tensorflow-rocm">Example - tensorflow-rocm</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#opencl-applications">OpenCL Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-blender-opencl">Example - Blender OpenCL</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">Licenses</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Apptainer User Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">GPU Support (NVIDIA CUDA &amp; AMD ROCm)</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/apptainer/apptainer-userdocs/blob/master/gpu.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="gpu-support-nvidia-cuda-amd-rocm">
<span id="gpu"></span><h1>GPU Support (NVIDIA CUDA &amp; AMD ROCm)<a class="headerlink" href="#gpu-support-nvidia-cuda-amd-rocm" title="Permalink to this heading"></a></h1>
<p>Apptainer natively supports running application containers that use
NVIDIA’s CUDA GPU compute framework, or AMD’s ROCm solution. This allows
easy access to users of GPU-enabled machine learning frameworks such as
TensorFlow, regardless of the host operating system. As long as the host
has a driver and library installation for CUDA/ROCm, then it’s possible
to e.g. run TensorFlow in an up-to-date Ubuntu 20.04 container, from an
older RHEL 7 host. However, note the libc version present in the container
and on the host shouldn’t be too far apart, especially if the version in
the container is newer than the version on the host, as this type of
mismatch can lead to a variety of low-level problems. Generally, running
a container with an older libc than the host will work, but the reverse
(running a container with a newer libc than the host) is more likely to
cause issues.</p>
<p>Applications that support OpenCL for compute acceleration can also be
used easily, with an additional bind option.</p>
<p>Apptainer experimental support is provided to use
Nvidia’s <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> tooling for GPU container setup. This
functionality, accessible via the new <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code> flag, improves
compatibility with OCI runtimes and exposes additional container
configuration options.</p>
<div class="section" id="nvidia-gpus-cuda-standard">
<h2>NVIDIA GPUs &amp; CUDA (Standard)<a class="headerlink" href="#nvidia-gpus-cuda-standard" title="Permalink to this heading"></a></h2>
<p>Commands that <code class="docutils literal notranslate"><span class="pre">run</span></code>, or otherwise execute containers (<code class="docutils literal notranslate"><span class="pre">shell</span></code>,
<code class="docutils literal notranslate"><span class="pre">exec</span></code>) can take an <code class="docutils literal notranslate"><span class="pre">--nv</span></code> option, which will setup the container’s
environment to use an NVIDIA GPU and the basic CUDA libraries to run a
CUDA enabled application. The <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag will:</p>
<ul class="simple">
<li><p>Ensure that the <code class="docutils literal notranslate"><span class="pre">/dev/nvidiaX</span></code> device entries are available inside
the container, so that the GPU cards in the host are accessible.</p></li>
<li><p>Locate and bind the basic CUDA libraries from the host into the
container, so that they are available to the container, and match the
kernel GPU driver on the host.</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> inside the container so that the bound-in
version of the CUDA libraries are used by applications run inside the
container.</p></li>
</ul>
<div class="section" id="requirements">
<h3>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading"></a></h3>
<p>To use the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag to run a CUDA application inside a container
you must ensure that:</p>
<ul class="simple">
<li><p>The host has a working installation of the NVIDIA GPU driver, and a
matching version of the basic NVIDIA/CUDA libraries. The host <em>does
not</em> need to have an X server running, unless you want to run
graphical apps from the container.</p></li>
<li><p>The NVIDIA libraries are in the system’s library search path.</p></li>
<li><p>The application inside your container was compiled for a CUDA
version, and device capability level, that is supported by the host
card and driver.</p></li>
</ul>
<p>These requirements are usually satisfied by installing the NVIDIA
drivers and CUDA packages directly from the NVIDIA website. Linux
distributions may provide NVIDIA drivers and CUDA libraries, but they
are often outdated which can lead to problems running applications
compiled for the latest versions of CUDA.</p>
<p>Apptainer will find the NVIDIA/CUDA libraries on your host using the
list of libraries in the configuration file
<code class="docutils literal notranslate"><span class="pre">etc/apptainer/nvbliblist</span></code>, and resolving paths through the
<code class="docutils literal notranslate"><span class="pre">ldconfig</span></code> cache. At time of release this list is appropriate for the
latest stable CUDA version. It can be modified by the administrator to
add additional libraries if necessary. See the admin guide for more
details.</p>
</div>
<div class="section" id="example-tensorflow-gpu">
<h3>Example - tensorflow-gpu<a class="headerlink" href="#example-tensorflow-gpu" title="Permalink to this heading"></a></h3>
<p>Tensorflow is commonly used for machine learning projects but can be
difficult to install on older systems, and is updated frequently.
Running tensorflow from a container removes installation problems and
makes trying out new versions easy.</p>
<p>The official tensorflow repository on Docker Hub contains NVIDA GPU
supporting containers, that will use CUDA for processing. You can view
the available versions on the <a class="reference external" href="https://hub.docker.com/r/tensorflow/tensorflow/tags">tags page on Docker Hub</a></p>
<p>The container is large, so it’s best to build or pull the docker image
to a SIF before you start working with it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer pull docker://tensorflow/tensorflow:latest-gpu
...
INFO:    Creating SIF file...
INFO:    Build complete: tensorflow_latest-gpu.sif
</pre></div>
</div>
<p>Then run the container with GPU support:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer run --nv tensorflow_latest-gpu.sif

________                               _______________
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

Apptainer&gt;
</pre></div>
</div>
<p>You can verify the GPU is available within the container by using the
tensorflow <code class="docutils literal notranslate"><span class="pre">list_local_devices()</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Apptainer</span><span class="o">&gt;</span> <span class="n">python</span>
<span class="n">Python</span> <span class="mf">2.7.15</span><span class="o">+</span> <span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">Jul</span>  <span class="mi">9</span> <span class="mi">2019</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span><span class="mi">51</span><span class="p">:</span><span class="mi">35</span><span class="p">)</span>
<span class="p">[</span><span class="n">GCC</span> <span class="mf">7.4.0</span><span class="p">]</span> <span class="n">on</span> <span class="n">linux2</span>
<span class="n">Type</span> <span class="s2">&quot;help&quot;</span><span class="p">,</span> <span class="s2">&quot;copyright&quot;</span><span class="p">,</span> <span class="s2">&quot;credits&quot;</span> <span class="ow">or</span> <span class="s2">&quot;license&quot;</span> <span class="k">for</span> <span class="n">more</span> <span class="n">information</span><span class="o">.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">device_lib</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">())</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.743600</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">platform</span><span class="o">/</span><span class="n">cpu_feature_guard</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">142</span><span class="p">]</span> <span class="n">Your</span> <span class="n">CPU</span> <span class="n">supports</span> <span class="n">instructions</span> <span class="n">that</span> <span class="n">this</span> <span class="n">TensorFlow</span> <span class="n">binary</span> <span class="n">was</span> <span class="ow">not</span> <span class="n">compiled</span> <span class="n">to</span> <span class="n">use</span><span class="p">:</span> <span class="n">AVX2</span> <span class="n">FMA</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.784482</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">platform</span><span class="o">/</span><span class="n">profile_utils</span><span class="o">/</span><span class="n">cpu_utils</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">94</span><span class="p">]</span> <span class="n">CPU</span> <span class="n">Frequency</span><span class="p">:</span> <span class="mi">3292620000</span> <span class="n">Hz</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.787911</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">168</span><span class="p">]</span> <span class="n">XLA</span> <span class="n">service</span> <span class="mh">0x565246634360</span> <span class="n">executing</span> <span class="n">computations</span> <span class="n">on</span> <span class="n">platform</span> <span class="n">Host</span><span class="o">.</span> <span class="n">Devices</span><span class="p">:</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.787939</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">175</span><span class="p">]</span>   <span class="n">StreamExecutor</span> <span class="n">device</span> <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Host</span><span class="p">,</span> <span class="n">Default</span> <span class="n">Version</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.798428</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">platform</span><span class="o">/</span><span class="n">default</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">44</span><span class="p">]</span> <span class="n">Successfully</span> <span class="n">opened</span> <span class="n">dynamic</span> <span class="n">library</span> <span class="n">libcuda</span><span class="o">.</span><span class="n">so</span><span class="mf">.1</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.842683</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">cuda_gpu_executor</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1006</span><span class="p">]</span> <span class="n">successful</span> <span class="n">NUMA</span> <span class="n">node</span> <span class="n">read</span> <span class="kn">from</span> <span class="nn">SysFS</span> <span class="n">had</span> <span class="n">negative</span> <span class="n">value</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">but</span> <span class="n">there</span> <span class="n">must</span> <span class="n">be</span> <span class="n">at</span> <span class="n">least</span> <span class="n">one</span> <span class="n">NUMA</span> <span class="n">node</span><span class="p">,</span> <span class="n">so</span> <span class="n">returning</span> <span class="n">NUMA</span> <span class="n">node</span> <span class="n">zero</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.843252</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">168</span><span class="p">]</span> <span class="n">XLA</span> <span class="n">service</span> <span class="mh">0x5652469263d0</span> <span class="n">executing</span> <span class="n">computations</span> <span class="n">on</span> <span class="n">platform</span> <span class="n">CUDA</span><span class="o">.</span> <span class="n">Devices</span><span class="p">:</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.843265</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">175</span><span class="p">]</span>   <span class="n">StreamExecutor</span> <span class="n">device</span> <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">GeForce</span> <span class="n">GT</span> <span class="mi">730</span><span class="p">,</span> <span class="n">Compute</span> <span class="n">Capability</span> <span class="mf">3.5</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.843380</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">cuda_gpu_executor</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1006</span><span class="p">]</span> <span class="n">successful</span> <span class="n">NUMA</span> <span class="n">node</span> <span class="n">read</span> <span class="kn">from</span> <span class="nn">SysFS</span> <span class="n">had</span> <span class="n">negative</span> <span class="n">value</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">but</span> <span class="n">there</span> <span class="n">must</span> <span class="n">be</span> <span class="n">at</span> <span class="n">least</span> <span class="n">one</span> <span class="n">NUMA</span> <span class="n">node</span><span class="p">,</span> <span class="n">so</span> <span class="n">returning</span> <span class="n">NUMA</span> <span class="n">node</span> <span class="n">zero</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">15</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mf">09.843984</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">common_runtime</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_device</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1618</span><span class="p">]</span> <span class="n">Found</span> <span class="n">device</span> <span class="mi">0</span> <span class="k">with</span> <span class="n">properties</span><span class="p">:</span>
<span class="n">name</span><span class="p">:</span> <span class="n">GeForce</span> <span class="n">GT</span> <span class="mi">730</span> <span class="n">major</span><span class="p">:</span> <span class="mi">3</span> <span class="n">minor</span><span class="p">:</span> <span class="mi">5</span> <span class="n">memoryClockRate</span><span class="p">(</span><span class="n">GHz</span><span class="p">):</span> <span class="mf">0.9015</span>
<span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="multiple-gpus">
<h3>Multiple GPUs<a class="headerlink" href="#multiple-gpus" title="Permalink to this heading"></a></h3>
<p>By default, Apptainer makes all host devices available in the
container. When the <code class="docutils literal notranslate"><span class="pre">--contain</span></code> option is used a minimal <code class="docutils literal notranslate"><span class="pre">/dev</span></code> tree
is created in the container, but the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> option will ensure that
all nvidia devices on the host are present in the container.</p>
<p>This behaviour is different to <code class="docutils literal notranslate"><span class="pre">nvidia-docker</span></code> where an
<code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code> environment variable is used to control
whether some or all host GPUs are visible inside a container. The
<code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code> explicitly binds the devices into the
container dependent on the value of <code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code>.</p>
<p>To control which GPUs are used in an Apptainer container that is run
with <code class="docutils literal notranslate"><span class="pre">--nv</span></code> you can set <code class="docutils literal notranslate"><span class="pre">APPTAINERENV_CUDA_VISIBLE_DEVICES</span></code> before
running the container, or <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> inside the container.
This variable will limit the GPU devices that CUDA programs see.</p>
<p>E.g. to run the tensorflow container, but using only the first GPU in
the host, we could do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ APPTAINERENV_CUDA_VISIBLE_DEVICES=0 apptainer run --nv tensorflow_latest-gpu.sif

# or

$ export APPTAINERENV_CUDA_VISIBLE_DEVICES=0
$ apptainer run tensorflow_latest-gpu.sif
</pre></div>
</div>
</div>
<div class="section" id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this heading"></a></h3>
<p>If the host installation of the NVIDIA / CUDA driver and libraries is
working and up-to-date there are rarely issues running CUDA programs
inside of Apptainer containers. The most common issue seen is:</p>
<div class="section" id="cuda-error-unknown-when-everything-seems-to-be-correctly-configured">
<h4>CUDA_ERROR_UNKNOWN when everything seems to be correctly configured<a class="headerlink" href="#cuda-error-unknown-when-everything-seems-to-be-correctly-configured" title="Permalink to this heading"></a></h4>
<p>CUDA depends on multiple kernel modules being loaded. Not all of the
modules are loaded at system startup. Some portions of the NVIDA driver
stack are initialized when first needed. This is done using a setuid
root binary, so initializing can be triggered by any user on the host.
In Apptainer containers, privilege escalation is blocked, so the
setuid root binary cannot initialize the driver stack fully.</p>
<p>If you experience <code class="docutils literal notranslate"><span class="pre">CUDA_ERROR_UNKNOWN</span></code> in a container, initialize the
driver stack on the host first, by running a CUDA program there or
<code class="docutils literal notranslate"><span class="pre">modprobe</span> <span class="pre">nvidia_uvm</span></code> as root, and using <code class="docutils literal notranslate"><span class="pre">nvidia-persistenced</span></code> to
avoid driver unload.</p>
</div>
</div>
</div>
<div class="section" id="nvidia-gpus-cuda-nvidia-container-cli">
<h2>NVIDIA GPUs &amp; CUDA (nvidia-container-cli)<a class="headerlink" href="#nvidia-gpus-cuda-nvidia-container-cli" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code> option instructs
Apptainer to perform GPU container setup using the
<code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> utility. This utility must be installed
separately from Apptainer. It is available in the repositories of
some distributions, and at:
<a class="reference external" href="https://nvidia.github.io/libnvidia-container/">https://nvidia.github.io/libnvidia-container/</a></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This feature is considered experimental in Apptainer as of now. It
cannot not replace the legacy NVIDIA support in all situations, and
should be tested carefully before use in production workflows.</p>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> to configure a container for GPU
operation has a number of advantages, including:</p>
<ul class="simple">
<li><p>The tool is maintained by NVIDIA, and will track new features /
libraries in new CUDA releases closely.</p></li>
<li><p>Support for passing only specific GPUs / MIG devices into the
container.</p></li>
<li><p>Support for providing different classes of GPU capability to the
container, e.g. compute, graphics, and display functionality.</p></li>
<li><p>Configuration via the same environment variables that are in use with
OCI containers.</p></li>
</ul>
<div class="section" id="requirements-limitations">
<h3>Requirements &amp; Limitations<a class="headerlink" href="#requirements-limitations" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> must be installed on your host.
It must be able to be found by the <code class="docutils literal notranslate"><span class="pre">binary</span> <span class="pre">path</span></code> set in
<code class="docutils literal notranslate"><span class="pre">apptainer.conf</span></code> which by default includes the user’s PATH.</p></li>
<li><p>For security reasons, <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code> cannot be used with
privileged mode in a setuid install of Apptainer.
<code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> also requires writing to the image, so
either the <code class="docutils literal notranslate"><span class="pre">--writable</span></code> (<code class="docutils literal notranslate"><span class="pre">-w</span></code>) or <code class="docutils literal notranslate"><span class="pre">--writable-tmpfs</span></code> option
is also required; if neither is given, <code class="docutils literal notranslate"><span class="pre">--writable-tmpfs</span></code> is
implied.
That also means that the permissions on system directories such as
<code class="docutils literal notranslate"><span class="pre">/usr/bin</span></code> have to be writable, so either use a sandbox image that
has that directory writable by the user (for example built with
the <code class="docutils literal notranslate"><span class="pre">--fix-perms</span></code> option) or use <code class="docutils literal notranslate"><span class="pre">--fakeroot</span></code>.</p></li>
<li><p>There are known problems with library discovery for the current
<code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> in recent Debian distributions. See <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/issues/1399">this
GitHub issue</a></p></li>
</ul>
</div>
<div class="section" id="id1">
<h3>Example - tensorflow-gpu<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<p>Tensorflow can be run using <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code> in a similar manner as the
standard <code class="docutils literal notranslate"><span class="pre">--nv</span></code> binding approach when run unprivleged. Build the
large container into a sandbox:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer build --sandbox tensorflow_latest-gpu docker://tensorflow/tensorflow:latest-gpu
INFO:    Starting build...
...
INFO:    Creating sandbox directory...
INFO:    Build complete: tensorflow_latest-gpu
</pre></div>
</div>
<p>Then run the container with <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> GPU support:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer run --nvccli tensorflow_latest-gpu

________                               _______________
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

Apptainer&gt;
</pre></div>
</div>
<p>You can verify the GPU is available within the container by using the
tensorflow <code class="docutils literal notranslate"><span class="pre">list_local_devices()</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Apptainer</span><span class="o">&gt;</span> <span class="n">python</span>
<span class="n">Python</span> <span class="mf">2.7.15</span><span class="o">+</span> <span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">Jul</span>  <span class="mi">9</span> <span class="mi">2019</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span><span class="mi">51</span><span class="p">:</span><span class="mi">35</span><span class="p">)</span>
<span class="p">[</span><span class="n">GCC</span> <span class="mf">7.4.0</span><span class="p">]</span> <span class="n">on</span> <span class="n">linux2</span>
<span class="n">Type</span> <span class="s2">&quot;help&quot;</span><span class="p">,</span> <span class="s2">&quot;copyright&quot;</span><span class="p">,</span> <span class="s2">&quot;credits&quot;</span> <span class="ow">or</span> <span class="s2">&quot;license&quot;</span> <span class="k">for</span> <span class="n">more</span> <span class="n">information</span><span class="o">.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">device_lib</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">())</span>
<span class="o">...</span>
<span class="n">device_type</span><span class="p">:</span> <span class="s2">&quot;GPU&quot;</span>
<span class="n">memory_limit</span><span class="p">:</span> <span class="mi">14474280960</span>
<span class="n">locality</span> <span class="p">{</span>
  <span class="n">bus_id</span><span class="p">:</span> <span class="mi">1</span>
  <span class="n">links</span> <span class="p">{</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">incarnation</span><span class="p">:</span> <span class="mi">13349913758992036690</span>
<span class="n">physical_device_desc</span><span class="p">:</span> <span class="s2">&quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;</span>
<span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="gpu-selection">
<h3>GPU Selection<a class="headerlink" href="#gpu-selection" title="Permalink to this heading"></a></h3>
<p>When running with <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code>, by default Apptainer will expose all
GPUs on the host inside the container. This mirrors the functionality of
the standard GPU support for the most common use-case.</p>
<p>Setting the <code class="docutils literal notranslate"><span class="pre">APPTAINER_CUDA_VISIBLE_DEVICES</span></code> environment variable
before running a container is still supported, to control which GPUs are
used by CUDA programs that honor <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>. However, more
powerful GPU isolation is possible using the <code class="docutils literal notranslate"><span class="pre">--contain</span></code> (or <code class="docutils literal notranslate"><span class="pre">-c</span></code>) flag and
<code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code> environment variable. This controls which GPU
devices are bound into the <code class="docutils literal notranslate"><span class="pre">/dev</span></code> tree in the container.</p>
<p>For example, to pass only the 2nd and 3rd GPU into a container running
on a system with 4 GPUs, run the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ export NVIDIA_VISIBLE_DEVICES=1,2
$ apptainer run -uwc --nvccli tensorflow_latest-gpu
</pre></div>
</div>
<p>Note that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code> is not prepended with <code class="docutils literal notranslate"><span class="pre">APPTAINER_</span></code> as
this variable controls container setup, and is not passed into the
container.</p></li>
<li><p>The GPU device identifiers start at 0, so 1,2 refers to the 2nd and
3rd GPU.</p></li>
<li><p>You can use GPU UUIDs in place of numeric identifiers. Use
<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">-L</span></code> to list both numeric IDs and UUIDs available on the
system.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">all</span></code> can be used to pass all available GPUs into the container.</p></li>
</ul>
<p>If you use <code class="docutils literal notranslate"><span class="pre">--contain</span></code> without setting <code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES</span></code>, no
GPUs will be available in the container, and a warning will be shown:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer run -uwc --nvccli tensorflow_latest-gpu
WARNING: When using nvidia-container-cli with --contain NVIDIA_VISIBLE_DEVICES
must be set or no GPUs will be available in container.
</pre></div>
</div>
<p>To restore the behaviour of the standard GPU handling, set
<code class="docutils literal notranslate"><span class="pre">NVIDIA_VISIBLE_DEVICES=0</span></code> when running with <code class="docutils literal notranslate"><span class="pre">--contain</span></code>.</p>
<p>If your system contains Ampere or newer GPUs that support virtual MIG
devices, you can specify MIG identifiers / UUIDs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ export NVIDIA_VISIBLE_DEVICES=MIG-GPU-5c89852c-d268-c3f3-1b07-005d5ae1dc3f/7/0
</pre></div>
</div>
<p>Apptainer does not configure MIG partitions. It is expected that
these would be statically configured by the system administrator, or
setup dynamically by a job scheduler / workflow system according to the
requirements of the job.</p>
</div>
<div class="section" id="other-gpu-options">
<h3>Other GPU Options<a class="headerlink" href="#other-gpu-options" title="Permalink to this heading"></a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">--nvccli</span></code> mode, Apptainer understands the following additional
environment variables. Note that these environment variables are read
from the environment where <code class="docutils literal notranslate"><span class="pre">apptainer</span></code> is run. Apptainer does
not currently read these settings from the container environment.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA_DRIVER_CAPABILITIES</span></code> controls which libraries and utilities
are mounted in the container, to support different requirements. The
default value under Apptainer is <code class="docutils literal notranslate"><span class="pre">compute,utility</span></code>, which will
provide CUDA functionality and basic utilities such as
<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>. Other options include <code class="docutils literal notranslate"><span class="pre">graphics</span></code> for OpenGL/Vulkan
support, <code class="docutils literal notranslate"><span class="pre">video</span></code> for the codecs SDK, and <code class="docutils literal notranslate"><span class="pre">display</span></code> to use X11
from a container.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA_REQUIRE_*</span></code> variables allow specifying requirements, which
will be checked by <code class="docutils literal notranslate"><span class="pre">nvidia-container-cli</span></code> prior to starting the
container. Constraints can be set on <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">driver</span></code>, <code class="docutils literal notranslate"><span class="pre">arch</span></code>,
and <code class="docutils literal notranslate"><span class="pre">brand</span></code> values. Docker/OCI images may set these variables
inside the container, to indicate runtime requirements. However,
these container variables are not yet interpreted by Apptainer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA_DISABLE_REQUIRE</span></code> will disable the enforcement of any
<code class="docutils literal notranslate"><span class="pre">NVIDIA_REQUIRE_*</span></code> requirements that are set.</p></li>
</ul>
<p>Full details of the supported values for these environment variables can
be found in the container-toolkit guide:</p>
<p><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#environment-variables-oci-spec">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#environment-variables-oci-spec</a></p>
</div>
</div>
<div class="section" id="amd-gpus-rocm">
<h2>AMD GPUs &amp; ROCm<a class="headerlink" href="#amd-gpus-rocm" title="Permalink to this heading"></a></h2>
<p>Apptainer has a <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> flag to support GPU compute with the
ROCm framework using AMD Radeon GPU cards.</p>
<p>Commands that <code class="docutils literal notranslate"><span class="pre">run</span></code>, or otherwise execute containers (<code class="docutils literal notranslate"><span class="pre">shell</span></code>,
<code class="docutils literal notranslate"><span class="pre">exec</span></code>) can take an <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> option, which will setup the
container’s environment to use a Radeon GPU and the basic ROCm libraries
to run a ROCm enabled application. The <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> flag will:</p>
<ul class="simple">
<li><p>Ensure that the <code class="docutils literal notranslate"><span class="pre">/dev/dri/</span></code> device entries are available inside the
container, so that the GPU cards in the host are accessible.</p></li>
<li><p>Locate and bind the basic ROCm libraries from the host into the
container, so that they are available to the container, and match the
kernel GPU driver on the host.</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> inside the container so that the bound-in
version of the ROCm libraries are used by application run inside the
container.</p></li>
</ul>
<div class="section" id="id2">
<h3>Requirements<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<p>To use the <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> flag to run a CUDA application inside a container
you must ensure that:</p>
<ul class="simple">
<li><p>The host has a working installation of the <code class="docutils literal notranslate"><span class="pre">amdgpu</span></code> driver, and a
compatible version of the basic ROCm libraries. The host <em>does not</em>
need to have an X server running, unless you want to run graphical
apps from the container.</p></li>
<li><p>The ROCm libraries are in the system’s library search path.</p></li>
<li><p>The application inside your container was compiled for a ROCm version
that is compatible with the ROCm version on your host.</p></li>
</ul>
<p>These requirements can be satisfied by following the requirements on the
<a class="reference external" href="https://rocm.github.io/ROCmInstall.html">ROCm web site</a></p>
</div>
<div class="section" id="example-tensorflow-rocm">
<h3>Example - tensorflow-rocm<a class="headerlink" href="#example-tensorflow-rocm" title="Permalink to this heading"></a></h3>
<p>Tensorflow is commonly used for machine learning projects, but can be
difficult to install on older systems, and is updated frequently.
Running tensorflow from a container removes installation problems and
makes trying out new versions easy.</p>
<p>The rocm tensorflow repository on Docker Hub contains Radeon GPU
supporting containers, that will use ROCm for processing. You can view
the available versions on the <a class="reference external" href="https://hub.docker.com/r/rocm/tensorflow/tags">tags page on Docker Hub</a></p>
<p>The container is large, so it’s best to build or pull the docker image
to a SIF before you start working with it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer pull docker://rocm/tensorflow:latest
...
INFO:    Creating SIF file...
INFO:    Build complete: tensorflow_latest.sif
</pre></div>
</div>
<p>Then run the container with GPU support:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer run --rocm tensorflow_latest.sif
</pre></div>
</div>
<p>You can verify the GPU is available within the container by using the
tensorflow <code class="docutils literal notranslate"><span class="pre">list_local_devices()</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Apptainer</span><span class="o">&gt;</span> <span class="n">ipython</span>
<span class="n">Python</span> <span class="mf">3.5.2</span> <span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">Jul</span> <span class="mi">10</span> <span class="mi">2019</span><span class="p">,</span> <span class="mi">11</span><span class="p">:</span><span class="mi">58</span><span class="p">:</span><span class="mi">48</span><span class="p">)</span>
<span class="n">Type</span> <span class="s1">&#39;copyright&#39;</span><span class="p">,</span> <span class="s1">&#39;credits&#39;</span> <span class="ow">or</span> <span class="s1">&#39;license&#39;</span> <span class="k">for</span> <span class="n">more</span> <span class="n">information</span>
<span class="n">IPython</span> <span class="mf">7.8.0</span> <span class="o">--</span> <span class="n">An</span> <span class="n">enhanced</span> <span class="n">Interactive</span> <span class="n">Python</span><span class="o">.</span> <span class="n">Type</span> <span class="s1">&#39;?&#39;</span> <span class="k">for</span> <span class="n">help</span><span class="o">.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">device_lib</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">())</span>
<span class="o">...</span>
<span class="mi">2019</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">14</span> <span class="mi">16</span><span class="p">:</span><span class="mi">33</span><span class="p">:</span><span class="mf">42.750509</span><span class="p">:</span> <span class="n">I</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">common_runtime</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_device</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1651</span><span class="p">]</span> <span class="n">Found</span> <span class="n">device</span> <span class="mi">0</span> <span class="k">with</span> <span class="n">properties</span><span class="p">:</span>
<span class="n">name</span><span class="p">:</span> <span class="n">Lexa</span> <span class="n">PRO</span> <span class="p">[</span><span class="n">Radeon</span> <span class="n">RX</span> <span class="mi">550</span><span class="o">/</span><span class="mi">550</span><span class="n">X</span><span class="p">]</span>
<span class="n">AMDGPU</span> <span class="n">ISA</span><span class="p">:</span> <span class="n">gfx803</span>
<span class="n">memoryClockRate</span> <span class="p">(</span><span class="n">GHz</span><span class="p">)</span> <span class="mf">1.183</span>
<span class="n">pciBusID</span> <span class="mi">0000</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mf">00.0</span>
<span class="o">...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="opencl-applications">
<h2>OpenCL Applications<a class="headerlink" href="#opencl-applications" title="Permalink to this heading"></a></h2>
<p>Both the <code class="docutils literal notranslate"><span class="pre">--rocm</span></code> and <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flags will bind the vendor OpenCL
implementation libraries into a container that is being run. However,
these libraries will not be used by OpenCL applications unless a vendor
icd file is available under <code class="docutils literal notranslate"><span class="pre">/etc/OpenCL/vendors</span></code> that directs OpenCL
to use the vendor library.</p>
<p>The simplest way to use OpenCL in a container is to <code class="docutils literal notranslate"><span class="pre">--bind</span>
<span class="pre">/etc/OpenCL</span></code> so that the icd files from the host (which match the
bound-in libraries) are present in the container.</p>
<div class="section" id="example-blender-opencl">
<h3>Example - Blender OpenCL<a class="headerlink" href="#example-blender-opencl" title="Permalink to this heading"></a></h3>
<p>The <a class="reference external" href="https://github.com/sylabs/examples">Sylabs examples repository</a>
contains an example container definition for the 3D modelling
application ‘Blender’.</p>
<p>The latest versions of Blender supports OpenCL rendering. You can run
Blender as a graphical application that will make use of a local Radeon
GPU for OpenCL compute using the container that has been published to
the Sylabs library:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ apptainer exec --rocm --bind /etc/OpenCL library://sylabs/examples/blender blender
</pre></div>
</div>
<p>Note the <em>exec</em> used as the <em>runscript</em> for this container is setup for
batch rendering (which can also use OpenCL).</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mpi.html" class="btn btn-neutral float-left" title="Apptainer and MPI applications" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="contributing.html" class="btn btn-neutral float-right" title="Contributing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
    <br>
    &copy; Contributors to the Apptainer project, established as Apptainer a Series of LF Projects LLC
    <br>
    &copy; 2017-2022, Sylabs Inc


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>